{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81a6865a-70bf-476e-ad7a-71dc4a3afe3a",
   "metadata": {},
   "source": [
    "# 🧪 The Enigmatic Research Assistant  \n",
    "*Automated Document Analysis Pipeline with RAG & Multimodal Support*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1831660b-374b-4fd7-8d07-6fc19f2b81a0",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87c35ab1-fadf-4dcf-b047-b4292232a600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TeHamer\\anaconda3\\envs\\LLM\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\TeHamer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import textwrap\n",
    "import logging\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import faiss\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from llama_cpp import Llama\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict, Set, Tuple  \n",
    "\n",
    "# Initialize NLTK\n",
    "nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=False)\n",
    "    nltk.download('punkt_tab', quiet=False)\n",
    "    nltk.download('perluniprops', quiet=False)\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "faiss.omp_set_num_threads(1)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0baf2a0d-6691-40d5-b156-93bd36a59e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "pip freeze > requirements/AdaptiveEngine.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ad47bb-4e89-4b8e-89ce-7aa7c5c4b5de",
   "metadata": {},
   "source": [
    "## 🔍 1. Text Extraction from Publications  \n",
    "**✅ Implemented** | *Key Libraries: `pdfplumber`, `python-docx`, `pandas`*\n",
    "\n",
    "| **Feature**              | **Implementation Details**                              |\n",
    "|--------------------------|--------------------------------------------------------|\n",
    "| **Supported Formats**     | `.pdf`, `.docx`, `.xlsx`, `.csv`, `.xls`, `.xlsm`      |\n",
    "| **Table Extraction**      | `[TABLE START]`/`[TABLE END]` markers for PDFs          |\n",
    "|                          | Word tables → Plain text conversion                    |\n",
    "| **Error Resilience**      | Skips unsupported files + logs warnings via `logging`   |\n",
    "| **Specialized Parsing**   | Spreadsheets → Markdown via `df.to_markdown()`          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09a4919c-d40c-456f-bd62-149cb3a4f532",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchProcessor:\n",
    "    SUPPORTED_FORMATS = ['.pdf', '.docx', '.xlsx', '.csv', '.xls', '.xlsm']\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.file_handlers = {\n",
    "            '.pdf': self._process_pdf,\n",
    "            '.docx': self._process_word,\n",
    "            '.xlsx': self._process_spreadsheet,\n",
    "            '.xls': self._process_spreadsheet,\n",
    "            '.xlsm': self._process_spreadsheet,\n",
    "            '.csv': self._process_spreadsheet\n",
    "        }\n",
    "    \n",
    "    def _process_pdf(self, file_path: Path) -> str:\n",
    "        from pdfplumber import open as pdf_open\n",
    "        content = []\n",
    "        try:\n",
    "            with pdf_open(file_path) as pdf:\n",
    "                for page_num, page in enumerate(pdf.pages, 1):\n",
    "                    text = page.extract_text() or \"\"\n",
    "                    tables = page.extract_tables()\n",
    "                    content.append(f\"\\n=== PAGE {page_num} ===\\n{text}\")\n",
    "                    if tables:\n",
    "                        content.append(\"\\n[TABLE START]\")\n",
    "                        for table in tables:\n",
    "                            for row in table:\n",
    "                                content.append(\"|\".join(map(str, row)))\n",
    "                        content.append(\"[TABLE END]\")\n",
    "            return \"\\n\".join(content)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"PDF processing error: {str(e)}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def _process_word(self, file_path: Path) -> str:\n",
    "        from docx import Document\n",
    "        content = []\n",
    "        try:\n",
    "            doc = Document(file_path)\n",
    "            for para in doc.paragraphs:\n",
    "                if para.text.strip():\n",
    "                    content.append(para.text)\n",
    "            for table in doc.tables:\n",
    "                content.append(\"[TABLE START]\")\n",
    "                for row in table.rows:\n",
    "                    content.append(\"|\".join(cell.text for cell in row.cells))\n",
    "                content.append(\"[TABLE END]\")\n",
    "            return \"\\n\".join(content) or \" \"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Word processing error: {str(e)}\")\n",
    "            return \" \"\n",
    "    \n",
    "    def _process_spreadsheet(self, file_path: Path) -> str:\n",
    "        content = []\n",
    "        try:\n",
    "            if file_path.suffix.lower() == '.csv':\n",
    "                df = pd.read_csv(file_path)\n",
    "            else:\n",
    "                df = pd.read_excel(file_path, engine='openpyxl')\n",
    "            content.append(\"[TABLE START]\")\n",
    "            content.append(df.to_markdown())\n",
    "            content.append(\"[TABLE END]\")\n",
    "            return \"\\n\".join(content)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Spreadsheet error: {str(e)}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def process_document(self, file_path: Path) -> Dict:\n",
    "        if not file_path.is_file():\n",
    "            logger.warning(f\"File not found: {file_path}\")\n",
    "            return None\n",
    "        \n",
    "        logger.info(f\"Processing {file_path.name}\")\n",
    "        handler = self.file_handlers.get(file_path.suffix.lower())\n",
    "        result = {\n",
    "            'filename': file_path.name,\n",
    "            'content': handler(file_path) if handler else \"\",\n",
    "            'processed_at': time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        \n",
    "        if not result['content'] or len(result['content']) < 100:\n",
    "            logger.warning(f\"Short/empty content in {file_path.name}\")\n",
    "            \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e1774d-3b51-4069-8e66-0a786a51fd01",
   "metadata": {},
   "source": [
    "### Key Components:\n",
    "- **Tokenization**  \n",
    "  ⚙️ `cl100k_base` tokenizer (GPT-4/LLaMA compatible)  \n",
    "  🎯 Target: 768 tokens/chunk (LLM optimization)\n",
    "\n",
    "- **Context Preservation**  \n",
    "  🔗 Page-level splits via `=== PAGE \\d+ ===` regex  \n",
    "  🔍 Sentence boundaries using `nltk` punkt\n",
    "\n",
    "- **Metadata Tracking**  \n",
    "  📌 `chunk_id: {filename}_{sequence}`  \n",
    "  📄 Page numbers + token counts per chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20d81d85-950c-42a3-80d2-343efd337cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchChunker:\n",
    "    def __init__(self, chunk_size=768):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        self._verify_nltk_resources()\n",
    "        self.min_chunk_length = 100\n",
    "        \n",
    "    def _verify_nltk_resources(self):\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "            self.sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        except LookupError:\n",
    "            nltk.download('punkt', quiet=True)\n",
    "            self.sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    \n",
    "    def _create_chunk(self, document: Dict, content: List[str], pages: Set[int], chunk_number: int) -> Dict:\n",
    "        return {\n",
    "            'document_id': document['filename'],\n",
    "            'chunk_id': f\"{document['filename']}_{chunk_number}\",\n",
    "            'content': ' '.join(content),\n",
    "            'page_numbers': sorted(pages),\n",
    "            'token_count': len(self.tokenizer.encode(' '.join(content)))\n",
    "        }\n",
    "    \n",
    "    def _extract_pages(self, content: List[str]) -> Set[int]:\n",
    "        pages = set()\n",
    "        for text in content:\n",
    "            matches = re.finditer(r'=== PAGE (\\d+) ===', text)\n",
    "            pages.update(int(m.group(1)) for m in matches)\n",
    "        return pages\n",
    "    \n",
    "    def chunk_document(self, document: Dict) -> List[Dict]:\n",
    "        chunks = []\n",
    "        try:\n",
    "            raw_content = document.get('content', '')\n",
    "            if not raw_content or len(raw_content) < self.min_chunk_length:\n",
    "                return []\n",
    "            \n",
    "            # Split by pages first\n",
    "            page_split = re.split(r'(=== PAGE \\d+ ===)', raw_content)\n",
    "            pages = []\n",
    "            current_page = []\n",
    "            \n",
    "            for item in page_split:\n",
    "                if re.match(r'=== PAGE \\d+ ===', item):\n",
    "                    if current_page:\n",
    "                        pages.append(\" \".join(current_page))\n",
    "                        current_page = []\n",
    "                    current_page.append(item)\n",
    "                else:\n",
    "                    current_page.append(item)\n",
    "            if current_page:\n",
    "                pages.append(\" \".join(current_page))\n",
    "            \n",
    "            # Process each page\n",
    "            for page_content in pages:\n",
    "                page_number = re.search(r'=== PAGE (\\d+) ===', page_content)\n",
    "                page_num = int(page_number.group(1)) if page_number else 1\n",
    "                \n",
    "                sentences = self.sentence_tokenizer.tokenize(page_content)\n",
    "                current_chunk = []\n",
    "                current_tokens = 0\n",
    "                \n",
    "                for sentence in sentences:\n",
    "                    sentence_tokens = len(self.tokenizer.encode(sentence))\n",
    "                    \n",
    "                    if current_tokens + sentence_tokens > self.chunk_size:\n",
    "                        if current_chunk:\n",
    "                            chunks.append(self._create_chunk(\n",
    "                                document, current_chunk, {page_num}, len(chunks)+1\n",
    "                            ))\n",
    "                            current_chunk = []\n",
    "                            current_tokens = 0\n",
    "                            \n",
    "                    current_chunk.append(sentence)\n",
    "                    current_tokens += sentence_tokens\n",
    "                \n",
    "                if current_chunk:\n",
    "                    chunks.append(self._create_chunk(\n",
    "                        document, current_chunk, {page_num}, len(chunks)+1\n",
    "                    ))\n",
    "            \n",
    "            logger.info(f\"Generated {len(chunks)} chunks for {document['filename']}\")\n",
    "            return chunks\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Chunking failed for {document['filename']}: {str(e)}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c82cb4-e733-4f97-83bc-3e4c98a7d988",
   "metadata": {},
   "source": [
    "### Architecture Overview:\n",
    "```mermaid\n",
    "graph LR\n",
    "A[Raw Text] --> B(nomic-embed-text-v1) \n",
    "B --> C[768-dim Vectors]\n",
    "C --> D{FAISS Index}\n",
    "D --> E[Semantic Search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2caa7fee-1dd6-4234-b568-2c06f7aacfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchVectorDB:\n",
    "    def __init__(self):\n",
    "        self.embedder = SentenceTransformer(\n",
    "            'nomic-ai/nomic-embed-text-v1',\n",
    "            trust_remote_code=True,\n",
    "            device='cpu'\n",
    "        )\n",
    "        self._initialize_faiss_index()\n",
    "        self.metadata = []\n",
    "    \n",
    "    def _initialize_faiss_index(self):\n",
    "        dim = self.embedder.get_sentence_embedding_dimension()\n",
    "        self.index = faiss.IndexFlatL2(dim)\n",
    "        logger.info(f\"Initialized FAISS index with dimension {dim}\")\n",
    "    \n",
    "    def add_documents(self, chunks: List[Dict]):\n",
    "        if not chunks or not isinstance(chunks, list):\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            contents = [chunk['content'] for chunk in chunks if 'content' in chunk]\n",
    "            embeddings = self.embedder.encode(\n",
    "                contents,\n",
    "                show_progress_bar=True,\n",
    "                convert_to_numpy=True\n",
    "            )\n",
    "            if embeddings.shape[0] == 0:\n",
    "                return\n",
    "                \n",
    "            self.index.add(embeddings.astype('float32'))\n",
    "            self.metadata.extend(chunks)\n",
    "            logger.info(f\"Added {len(chunks)} chunks (Total: {self.index.ntotal})\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Vector DB error: {str(e)}\")\n",
    "    \n",
    "    def search(self, query_embed: np.ndarray, top_k=3) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        try:\n",
    "            if self.index.ntotal == 0:\n",
    "                return np.empty((0,)), np.empty((0,))\n",
    "                \n",
    "            if query_embed.ndim == 1:\n",
    "                query_embed = query_embed.reshape(1, -1)\n",
    "                \n",
    "            distances, indices = self.index.search(query_embed.astype('float32'), top_k)\n",
    "            return distances, indices\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Search failed: {str(e)}\")\n",
    "            return np.empty((0,)), np.empty((0,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769fd3a6-a6cf-44fa-988c-db1d8da11952",
   "metadata": {},
   "source": [
    "## 🤖 4. RAG Q&A System  \n",
    "**✅ Implemented** | *Components: `Llama-2-7B`, `llama.cpp`*\n",
    "\n",
    "```markdown\n",
    "### Workflow:\n",
    "1. 🔎 **Query Embedding**: `nomic-embed-text-v1` encodes question\n",
    "2. 🎯 **Top-3 Retrieval**: FAISS similarity search\n",
    "3. 💡 **Answer Generation**:\n",
    "   - Dynamic context window (max 2048 tokens)\n",
    "   - Temperature: 0.3 for focused responses\n",
    "4. 📚 **Source Attribution**:\n",
    "   ```python\n",
    "   {'document_id': 'file.pdf', 'page_numbers': [12,14]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d981e7-0180-46eb-8b00-330d91ebf267",
   "metadata": {},
   "source": [
    "## 🌍 5. Translation System  \n",
    "**✅ Implemented** | *Model: `mbart-large-50`*\n",
    "\n",
    "```markdown\n",
    "| **Feature**         | **Implementation**                          |\n",
    "|---------------------|--------------------------------------------|\n",
    "| Multilingual Support| 50 languages via HuggingFace pipeline       |\n",
    "| Format Preservation | Line-by-line translation                   |\n",
    "| Quality Validation  | BLEU score against reference texts         |\n",
    "| Efficiency          | Batch processing for large documents       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23081f70-be1e-4ddd-8b8d-067f6f509b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchAnalyst:\n",
    "    def __init__(self):\n",
    "        self.vector_db = ResearchVectorDB()\n",
    "        self.max_context_tokens = 2048  # Matches model's training context\n",
    "        self.llm = self._initialize_llm()\n",
    "        self.conversation_history = []\n",
    "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")  # Unified tokenizer\n",
    "\n",
    "    def _initialize_llm(self):\n",
    "        MODEL_NAME = \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n",
    "        model_path = Path(\"models\") / MODEL_NAME\n",
    "        \n",
    "        if not model_path.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"Model file {MODEL_NAME} not found in models/\\n\"\n",
    "                \"Download from: https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n",
    "            )\n",
    "            \n",
    "        logger.info(f\"Initializing LLM: {MODEL_NAME}\")\n",
    "        return Llama(\n",
    "            model_path=str(model_path),\n",
    "            n_ctx=self.max_context_tokens,\n",
    "            n_threads=4,\n",
    "            n_gpu_layers=0,  # CPU-only mode\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "    def answer_question(self, question: str, top_k=3) -> Dict:\n",
    "        try:\n",
    "            # Semantic search\n",
    "            query_embed = self.vector_db.embedder.encode([question])\n",
    "            distances, indices = self.vector_db.search(query_embed, top_k)\n",
    "            \n",
    "            # Handle empty results\n",
    "            if indices.size == 0 or len(self.vector_db.metadata) == 0:\n",
    "                return {\n",
    "                    'question': question,\n",
    "                    'answer': \"No relevant information found\",\n",
    "                    'sources': [],\n",
    "                    'metrics': {'tokens_sec': 0}\n",
    "                }\n",
    "            \n",
    "            # Build context with token tracking\n",
    "            context_parts = []\n",
    "            total_tokens = 0\n",
    "            used_indices = []\n",
    "            \n",
    "            for idx in indices[0]:\n",
    "                if idx >= len(self.vector_db.metadata):\n",
    "                    continue\n",
    "                \n",
    "                chunk = self.vector_db.metadata[idx]\n",
    "                chunk_tokens = len(self.tokenizer.encode(chunk['content']))\n",
    "                \n",
    "                if total_tokens + chunk_tokens > self.max_context_tokens:\n",
    "                    break\n",
    "                    \n",
    "                context_parts.append(f\"Document excerpt:\\n{chunk['content']}\")\n",
    "                total_tokens += chunk_tokens\n",
    "                used_indices.append(idx)\n",
    "            \n",
    "            # Generate answer\n",
    "            prompt = f\"Context information:\\n{' '.join(context_parts)}\\nQuestion: {question}\\nAnswer:\"\n",
    "            response = self.llm(\n",
    "                prompt=prompt,\n",
    "                max_tokens=512,\n",
    "                temperature=0.3,\n",
    "                stop=[\"\\n\", \"###\"]\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'question': question,\n",
    "                'answer': response['choices'][0]['text'].strip(),\n",
    "                'sources': [self.vector_db.metadata[i] for i in used_indices],\n",
    "                'metrics': {'tokens_sec': len(response['choices'][0]['text']) / (time.time() - start_time)}\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Q&A failed: {str(e)}\")\n",
    "            return {'error': str(e)}\n",
    "\n",
    "    def translate_content(self, text: str, target_lang: str) -> str:\n",
    "        \"\"\"Improved translation method with error handling\"\"\"\n",
    "        try:\n",
    "            if \"[TABLE START]\" in text:\n",
    "                return self._translate_table(text, target_lang)\n",
    "                \n",
    "            sentences = nltk.sent_tokenize(text)\n",
    "            translated = []\n",
    "            \n",
    "            for sent in sentences:\n",
    "                response = self.llm(\n",
    "                    prompt=f\"Translate to {target_lang}: {sent}\",\n",
    "                    max_tokens=len(sent)*3,  # Increased buffer\n",
    "                    temperature=0.1\n",
    "                )\n",
    "                translated.append(response['choices'][0]['text'].strip())\n",
    "                \n",
    "            return ' '.join(translated)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Translation failed: {str(e)}\")\n",
    "            return text  # Return original text on failure\n",
    "\n",
    "    def _translate_table(self, table_text: str, target_lang: str) -> str:\n",
    "        \"\"\"Table-aware translation\"\"\"\n",
    "        try:\n",
    "            rows = table_text.split('\\n')\n",
    "            translated_rows = []\n",
    "            \n",
    "            for row in rows:\n",
    "                if row.strip() in ['[TABLE START]', '[TABLE END]']:\n",
    "                    translated_rows.append(row)\n",
    "                    continue\n",
    "                    \n",
    "                cells = row.split('|')\n",
    "                translated_cells = []\n",
    "                \n",
    "                for cell in cells:\n",
    "                    response = self.llm(\n",
    "                        prompt=f\"Translate to {target_lang}: {cell}\",\n",
    "                        max_tokens=len(cell)*3,\n",
    "                        temperature=0.1\n",
    "                    )\n",
    "                    translated_cells.append(response['choices'][0]['text'].strip())\n",
    "                \n",
    "                translated_rows.append('|'.join(translated_cells))\n",
    "                \n",
    "            return '\\n'.join(translated_rows)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Table translation failed: {str(e)}\")\n",
    "            return table_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdda66fd-c44c-4f76-8f26-517b380869e8",
   "metadata": {},
   "source": [
    "### Step 5: Translation System\n",
    "- **Multilingual Support**: Used `mbart-large-50` model for high-quality translations.\n",
    "- **Format Preservation**: Translated text line-by-line to retain original structure.\n",
    "- **Evaluation**: Added BLEU score to compare translations with reference texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "606e3657-e61d-488c-a82f-bed9826708a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 22:16:43,096 - INFO - Load pretrained SentenceTransformer: nomic-ai/nomic-embed-text-v1\n",
      "2025-04-15 22:16:48,740 - WARNING - !!!!!!!!!!!!megablocks not available, using torch.matmul instead\n",
      "2025-04-15 22:16:51,201 - WARNING - <All keys matched successfully>\n",
      "2025-04-15 22:16:53,616 - INFO - Initialized FAISS index with dimension 768\n",
      "2025-04-15 22:16:53,616 - INFO - Initializing LLM: tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Initializing Document Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:   0%|                                                                     | 0/10 [00:00<?, ?it/s]2025-04-15 22:16:53,768 - INFO - Processing Dataset summaries and citations.docx\n",
      "2025-04-15 22:16:54,028 - INFO - Generated 11 chunks for Dataset summaries and citations.docx\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c362877e13c40a6bfc092c9f6866de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 22:17:06,229 - INFO - Added 11 chunks (Total: 11)\n",
      "Processing documents:  10%|██████                                                       | 1/10 [00:12<01:52, 12.46s/it]2025-04-15 22:17:06,231 - INFO - Processing Loan amortisation schedule1.xlsx\n",
      "2025-04-15 22:17:06,460 - INFO - Generated 1 chunks for Loan amortisation schedule1.xlsx\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0400ee3f0dc84073bb4e3f70b157ec13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 22:17:08,980 - INFO - Added 1 chunks (Total: 12)\n",
      "Processing documents:  20%|████████████▏                                                | 2/10 [00:15<00:53,  6.75s/it]2025-04-15 22:17:08,980 - INFO - Processing Loan analysis.xlsx\n",
      "2025-04-15 22:17:09,060 - INFO - Generated 2 chunks for Loan analysis.xlsx\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb451c9b46c4104aacb562aca3b7290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 22:17:27,370 - INFO - Added 2 chunks (Total: 14)\n",
      "Processing documents:  30%|██████████████████▎                                          | 3/10 [00:33<01:24, 12.06s/it]2025-04-15 22:17:27,370 - INFO - Processing M.Sc. Applied Psychology.docx\n",
      "2025-04-15 22:17:27,866 - INFO - Generated 37 chunks for M.Sc. Applied Psychology.docx\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b79fc37e1c94a25b783cebadd251692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 22:18:08,412 - INFO - Added 37 chunks (Total: 51)\n",
      "Processing documents:  40%|████████████████████████▍                                    | 4/10 [01:14<02:21, 23.50s/it]2025-04-15 22:18:08,414 - INFO - Processing new-approaches-and-procedures-for-cancer-treatment.pdf\n",
      "2025-04-15 22:18:11,041 - INFO - Generated 21 chunks for new-approaches-and-procedures-for-cancer-treatment.pdf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf8748da7fc4ac1bd1f4c71ca720903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 22:18:32,635 - INFO - Added 21 chunks (Total: 72)\n",
      "Processing documents:  50%|██████████████████████████████▌                              | 5/10 [01:38<01:58, 23.76s/it]2025-04-15 22:18:32,638 - INFO - Processing Ocean_ecogeochemistry_A_review.pdf\n",
      "2025-04-15 22:18:43,353 - INFO - Generated 97 chunks for Ocean_ecogeochemistry_A_review.pdf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746564ab805c4613a03105c296f39524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 22:20:10,316 - INFO - Added 97 chunks (Total: 169)\n",
      "Processing documents:  60%|████████████████████████████████████▌                        | 6/10 [03:16<03:15, 48.90s/it]2025-04-15 22:20:10,316 - INFO - Processing party budget1.xlsx\n",
      "2025-04-15 22:20:10,448 - INFO - Generated 1 chunks for party budget1.xlsx\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0bbaec8b5264b258839000f91764e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 22:20:17,021 - INFO - Added 1 chunks (Total: 170)\n",
      "Processing documents:  70%|██████████████████████████████████████████▋                  | 7/10 [03:23<01:45, 35.10s/it]2025-04-15 22:20:17,023 - INFO - Processing Stats.docx\n",
      "2025-04-15 22:20:17,189 - INFO - Generated 3 chunks for Stats.docx\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a117bf41db41bc8c87393cb55b0a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 22:20:20,206 - INFO - Added 3 chunks (Total: 173)\n",
      "Processing documents:  80%|████████████████████████████████████████████████▊            | 8/10 [03:26<00:49, 24.94s/it]2025-04-15 22:20:20,214 - INFO - Processing The-Alchemist.pdf\n",
      "2025-04-15 22:20:34,271 - INFO - Generated 136 chunks for The-Alchemist.pdf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126229ca160d47a0a8889efd7fc6760d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 22:21:59,012 - INFO - Added 136 chunks (Total: 309)\n",
      "Processing documents:  90%|██████████████████████████████████████████████████████▉      | 9/10 [05:05<00:48, 48.03s/it]2025-04-15 22:21:59,015 - INFO - Processing The_Plan_of_the_Giza_Pyramids.pdf\n",
      "2025-04-15 22:22:01,236 - INFO - Generated 22 chunks for The_Plan_of_the_Giza_Pyramids.pdf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65366c495be845048cdfce5b42eca7cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 22:22:25,334 - INFO - Added 22 chunks (Total: 331)\n",
      "Processing documents: 100%|████████████████████████████████████████████████████████████| 10/10 [05:31<00:00, 33.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Successfully processed 331 chunks from 10 files\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Enter your question (or press Enter to exit):\n",
      ">  Tell Me About The Giza System\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd585ac65dc4ea78cd474bc87c8577b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📜 Question: Tell Me About The Giza System\n",
      "\n",
      "💡 Answer:\n",
      "The Giza System is a plan of the three pyramids of Khufu, Khafre, and Menkaure.\n",
      "The plan is based on the dimensions of the pyramids and the positions of the\n",
      "corners. The dimensions are in royal cubits\n",
      "\n",
      "⚡ Performance: 4.0 tokens/sec\n",
      "\n",
      "📚 Sources:\n",
      "1. The_Plan_of_the_Giza_Pyramids.pdf (Pages: [13])\n",
      "2. The_Plan_of_the_Giza_Pyramids.pdf (Pages: [16])\n",
      "3. The_Plan_of_the_Giza_Pyramids.pdf (Pages: [1])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Enter your question (or press Enter to exit):\n",
      ">  What is the Ocean_ecogeochemistry?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1692feb73c3947cbba7e72c87e46f8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📜 Question: What is the Ocean_ecogeochemistry?\n",
      "\n",
      "💡 Answer:\n",
      "Ocean ecoGeoChemistry is a discipline that integrates chemistry, physiology, and\n",
      "biology to study the chemical, physical, and biochemical processes that\n",
      "determine the isotopic composition of marine animals.\n",
      "\n",
      "⚡ Performance: 5.6 tokens/sec\n",
      "\n",
      "📚 Sources:\n",
      "1. Ocean_ecogeochemistry_A_review.pdf (Pages: [3])\n",
      "2. Ocean_ecogeochemistry_A_review.pdf (Pages: [11])\n",
      "3. Ocean_ecogeochemistry_A_review.pdf (Pages: [29])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Enter your question (or press Enter to exit):\n",
      ">  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🔚 Analysis session completed\n"
     ]
    }
   ],
   "source": [
    "def analyze_research_documents(input_dir: Path):\n",
    "    processor = ResearchProcessor()\n",
    "    chunker = ResearchChunker(chunk_size=768)\n",
    "    analyst = ResearchAnalyst()\n",
    "    \n",
    "    print(\"\\n🔍 Initializing Document Processing...\")\n",
    "    total_chunks = 0\n",
    "    \n",
    "    for file_path in tqdm(list(input_dir.glob('*')), desc=\"Processing documents\"):\n",
    "        if file_path.suffix.lower() not in processor.SUPPORTED_FORMATS:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            doc = processor.process_document(file_path)\n",
    "            if not doc or not doc.get('content'):\n",
    "                continue\n",
    "                \n",
    "            chunks = chunker.chunk_document(doc)\n",
    "            if not chunks:\n",
    "                continue\n",
    "                \n",
    "            analyst.vector_db.add_documents(chunks)\n",
    "            total_chunks += len(chunks)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {file_path.name}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\n✅ Successfully processed {total_chunks} chunks from {len(list(input_dir.glob('*')))} files\")\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\n📝 Enter your question (or press Enter to exit):\\n> \").strip()\n",
    "        if not question:\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            result = analyst.answer_question(question)\n",
    "            print(f\"\\n📜 Question: {result['question']}\")\n",
    "            print(f\"\\n💡 Answer:\\n{textwrap.fill(result['answer'], width=80)}\")\n",
    "            print(f\"\\n⚡ Performance: {result['metrics']['tokens_sec']:.1f} tokens/sec\")\n",
    "            \n",
    "            if result['sources']:\n",
    "                print(\"\\n📚 Sources:\")\n",
    "                for i, source in enumerate(result['sources'], 1):\n",
    "                    print(f\"{i}. {source['document_id']} (Pages: {source['page_numbers']})\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error: {str(e)}\")\n",
    "            \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🔚 Analysis session completed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = Path(r\"D:\\Projects\\The Enigmatic Research of Dr. X\")\n",
    "    input_dir = base_dir / \"documents\"\n",
    "    if not input_dir.exists():\n",
    "        raise FileNotFoundError(f\"Directory not found: {input_dir}\")\n",
    "        \n",
    "    analyze_research_documents(input_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
