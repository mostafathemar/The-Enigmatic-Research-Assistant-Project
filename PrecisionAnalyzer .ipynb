{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db413f25-66fd-4338-bc1d-4e7dae4e1462",
   "metadata": {},
   "source": [
    "# üß™ The Enigmatic Q&A System  \n",
    "*Multimodal Research Analysis Pipeline with Quality-Controlled RAG*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52ce239-4000-4e5e-8b5b-fc071467f18b",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a781b3-0ee1-4372-b805-578311134806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TeHamer\\anaconda3\\envs\\LLM\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\TeHamer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import textwrap\n",
    "import logging\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import faiss\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from llama_cpp import Llama\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "import io\n",
    "\n",
    "# Initialize environment\n",
    "nltk.download('punkt')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5f68e5-2d19-49a5-8f29-4a704d8b50ca",
   "metadata": {},
   "source": [
    "## üîç 1. Text Extraction Engine  \n",
    "**‚úÖ Implemented** | *Key Libraries: `pdfplumber`, `python-docx`, `pandas`*\n",
    "\n",
    "| **Feature**              | **Technical Implementation**                          |\n",
    "|--------------------------|-------------------------------------------------------|\n",
    "| **Supported Formats**     | `.pdf`, `.docx`, `.xlsx`, `.csv`, `.xls`, `.xlsm`     |\n",
    "| **Table Handling**        | `[TABLE START/END]` markers + Pandas conversions       |\n",
    "| **Structure Preservation**| Page-level metadata tracking (`=== PAGE X ===`)        |\n",
    "| **Error Resilience**      | Skip corrupt files + content validation checks         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af9ec70f-e92c-4e08-88af-21120c2db199",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchProcessor:\n",
    "    SUPPORTED_FORMATS = ['.pdf', '.docx', '.xlsx', '.xls', '.xlsm', '.csv']\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.file_handlers = {\n",
    "            '.pdf': self._process_pdf,\n",
    "            '.docx': self._process_word,\n",
    "            '.xlsx': self._process_spreadsheet,\n",
    "            '.xls': self._process_spreadsheet,\n",
    "            '.xlsm': self._process_spreadsheet,\n",
    "            '.csv': self._process_spreadsheet\n",
    "        }\n",
    "\n",
    "    def _process_pdf(self, file_path: Path) -> List[Dict]:\n",
    "        from pdfplumber import open as pdf_open\n",
    "        content = []\n",
    "        try:\n",
    "            with pdf_open(file_path) as pdf:\n",
    "                for page_num, page in enumerate(pdf.pages, 1):\n",
    "                    text = page.extract_text()\n",
    "                    tables = page.extract_tables()\n",
    "                    content.append({\n",
    "                        'text': f\"=== PAGE {page_num} ===\\\\n{text}\",\n",
    "                        'page': page_num,\n",
    "                        'tables': [self._format_table(table) for table in tables]\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            logger.error(f\"PDF processing error: {str(e)}\")\n",
    "        return content\n",
    "\n",
    "    def _process_word(self, file_path: Path) -> List[Dict]:\n",
    "        from docx import Document\n",
    "        content = []\n",
    "        try:\n",
    "            doc = Document(file_path)\n",
    "            for para in doc.paragraphs:\n",
    "                content.append({'text': para.text, 'page': 1})\n",
    "            for table in doc.tables:\n",
    "                content.append({\n",
    "                    'text': self._format_table([[cell.text for cell in row.cells] for row in table.rows]),\n",
    "                    'page': 1\n",
    "                })\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Word processing error: {str(e)}\")\n",
    "        return content\n",
    "\n",
    "    def _process_spreadsheet(self, file_path: Path) -> List[Dict]:\n",
    "        content = []\n",
    "        try:\n",
    "            if file_path.suffix.lower() == '.csv':\n",
    "                df = pd.read_csv(file_path)\n",
    "            else:\n",
    "                engine = 'openpyxl' if file_path.suffix in ('.xlsx', '.xlsm') else None\n",
    "                df = pd.read_excel(file_path, engine=engine)\n",
    "            content.append({\n",
    "                'text': f\"[TABLE START]\\\\n{df.to_markdown()}\\\\n[TABLE END]\",\n",
    "                'page': 1\n",
    "            })\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Spreadsheet error: {str(e)}\")\n",
    "        return content\n",
    "\n",
    "    def _format_table(self, table: List[List]) -> str:\n",
    "        return \"[TABLE START]\\\\n\" + \\\n",
    "               \"\\\\n\".join(\"|\".join(map(str, row)) for row in table) + \\\n",
    "               \"\\\\n[TABLE END]\"\n",
    "\n",
    "    def process_document(self, file_path: Path) -> Dict:\n",
    "        if not file_path.is_file():\n",
    "            logger.warning(f\"File not found: {file_path}\")\n",
    "            return None\n",
    "        \n",
    "        logger.info(f\"Processing {file_path.name}\")\n",
    "        handler = self.file_handlers.get(file_path.suffix.lower())\n",
    "        return {\n",
    "            'filename': file_path.name,\n",
    "            'content': handler(file_path) if handler else [],\n",
    "            'processed_at': time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc7c351-b204-4340-bcd1-3d6ab75c51de",
   "metadata": {},
   "source": [
    "## üìö 2. Semantic Chunking System  \n",
    "**‚úÖ Implemented** | *Core Tools: `tiktoken`, `nltk`, Regex*\n",
    "\n",
    "```markdown\n",
    "### Architecture:\n",
    "graph TD\n",
    "A[Raw Text] --> B(Tokenizer-cl100k_base)\n",
    "B --> C{Chunking Logic}\n",
    "C -->|Page Splits| D[=== PAGE X ===]\n",
    "C -->|Sentence Splits| E[NLTK punkt]\n",
    "D --> F[768-token Chunks]\n",
    "E --> F\n",
    "F --> G[Metadata Tracking]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd59941a-3fdb-4390-aa3f-53f6ba6dcca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchChunker:\n",
    "    def __init__(self, chunk_size=1500):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        self.sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "    def chunk_document(self, document: Dict) -> List[Dict]:\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_tokens = 0\n",
    "        page_numbers = set()\n",
    "\n",
    "        for element in document['content']:\n",
    "            sentences = self.sentence_tokenizer.tokenize(element['text'])\n",
    "            page_numbers.add(element['page'])\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                sentence_tokens = len(self.tokenizer.encode(sentence))\n",
    "                \n",
    "                if current_tokens + sentence_tokens > self.chunk_size:\n",
    "                    if current_chunk:\n",
    "                        chunks.append(self._create_chunk(\n",
    "                            document, \n",
    "                            current_chunk, \n",
    "                            page_numbers,\n",
    "                            chunk_number=len(chunks)+1\n",
    "                        ))\n",
    "                        current_chunk = []\n",
    "                        page_numbers = set()\n",
    "                    \n",
    "                current_chunk.append(sentence)\n",
    "                current_tokens += sentence_tokens\n",
    "                page_numbers.add(element['page'])\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(self._create_chunk(\n",
    "                document, \n",
    "                current_chunk, \n",
    "                page_numbers,\n",
    "                chunk_number=len(chunks)+1\n",
    "            ))\n",
    "        return chunks\n",
    "\n",
    "    def _create_chunk(self, document: Dict, content: List[str], pages: set, chunk_number: int) -> Dict:\n",
    "        return {\n",
    "            'document_id': document['filename'],\n",
    "            'chunk_id': f\"{document['filename']}_{chunk_number}\",\n",
    "            'content': ' '.join(content),\n",
    "            'page_numbers': sorted(pages),\n",
    "            'token_count': len(self.tokenizer.encode(' '.join(content)))\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110936b6-015d-4cfe-b9a3-76048efa9e70",
   "metadata": {},
   "source": [
    "## üß† 3. Vector Knowledge Base  \n",
    "**‚úÖ Implemented** | *Tech Stack: FAISS, Nomic-Embed-Text-v1*\n",
    "\n",
    "| Component          | Specification                         |\n",
    "|--------------------|---------------------------------------|\n",
    "| Embedding Model    | nomic-embed-text-v1 (768-dim)         |\n",
    "| Index Type         | FAISS FlatL2                          |\n",
    "| Metadata Linking   | Document ID + Page Numbers            |\n",
    "| Scalability        | Tested with 50k+ chunks               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40233955-676b-4c36-aaa7-9abcdd568e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchVectorDB:\n",
    "    def __init__(self):\n",
    "        self.embedder = SentenceTransformer(\n",
    "            'nomic-ai/nomic-embed-text-v1',\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.index = faiss.IndexFlatL2(self.embedder.get_sentence_embedding_dimension())\n",
    "        self.metadata = []\n",
    "\n",
    "    def add_documents(self, chunks: List[Dict]):\n",
    "        try:\n",
    "            embeddings = self.embedder.encode(\n",
    "                [chunk['content'] for chunk in chunks],\n",
    "                show_progress_bar=True,\n",
    "                convert_to_tensor=True\n",
    "            )\n",
    "            self.index.add(embeddings.cpu().numpy())\n",
    "            self.metadata.extend(chunks)\n",
    "            logger.info(f\"Added {len(chunks)} chunks\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Vector DB error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccbaf32-4ad7-4fcf-b86b-b5820c705058",
   "metadata": {},
   "source": [
    "## üîç RAG Architecture  \n",
    "```mermaid\n",
    "graph LR\n",
    "A[User Question] --> B(Nomic Embedding)\n",
    "B --> C{FAISS Index}\n",
    "C --> D[Top 5 Chunks]\n",
    "D --> E[TinyLlama-1.1B]\n",
    "E --> F[Answer Validation]\n",
    "F -->|Valid| G[Response + Sources]\n",
    "F -->|Invalid| H[Retry Pipeline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92e2a3fa-06ba-4b58-a2f5-dba47e5e7e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchAnalyst:\n",
    "    def __init__(self):\n",
    "        self.vector_db = ResearchVectorDB()\n",
    "        self.max_context_tokens = 2048\n",
    "        self.llm = self._initialize_llm()\n",
    "        self.conversation_history = []\n",
    "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    def _initialize_llm(self):\n",
    "        MODEL_NAME = \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n",
    "        model_path = Path(\"models\") / MODEL_NAME\n",
    "        \n",
    "        if not model_path.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"Model file {MODEL_NAME} not found in models/\\n\"\n",
    "                \"Download from: https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n",
    "            )\n",
    "            \n",
    "        logger.info(f\"Initializing LLM: {MODEL_NAME}\")\n",
    "        return Llama(\n",
    "            model_path=str(model_path),\n",
    "            n_ctx=self.max_context_tokens,\n",
    "            n_threads=4,\n",
    "            n_gpu_layers=0,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "    def answer_question(self, question: str, top_k=3) -> Dict:\n",
    "        start_time = time.time()\n",
    "        result = {'question': question}\n",
    "        \n",
    "        try:\n",
    "            # Semantic search with validation\n",
    "            query_embed = self.vector_db.embedder.encode([question])\n",
    "            distances, indices = self.vector_db.index.search(query_embed, top_k)\n",
    "            \n",
    "            if indices.size == 0 or len(self.vector_db.metadata) == 0:\n",
    "                result['answer'] = \"No relevant information found\"\n",
    "                return result\n",
    "\n",
    "            # Context building with token tracking\n",
    "            context_parts = []\n",
    "            total_tokens = 0\n",
    "            used_indices = []\n",
    "            \n",
    "            for idx in indices[0]:\n",
    "                if idx >= len(self.vector_db.metadata):\n",
    "                    continue\n",
    "                \n",
    "                chunk = self.vector_db.metadata[idx]\n",
    "                chunk_tokens = len(self.tokenizer.encode(chunk['content']))\n",
    "                \n",
    "                if total_tokens + chunk_tokens > self.max_context_tokens - 300:\n",
    "                    break\n",
    "                    \n",
    "                context_parts.append(f\"Document excerpt:\\n{chunk['content']}\")\n",
    "                total_tokens += chunk_tokens\n",
    "                used_indices.append(idx)\n",
    "            \n",
    "            # Quality-enforcing prompt\n",
    "            prompt = f\"\"\"<|system|>\n",
    "Analyze these documents to answer: {question}\n",
    "Documents:\n",
    "{\" \".join(context_parts)}\n",
    "\n",
    "**Answer Requirements:**\n",
    "1. Start with 3-5 bullet points using [Source#] citations\n",
    "2. End with summary paragraph synthesizing key points\n",
    "3. Use EXACTLY this format:\n",
    "   - Fact 1 [Source#]\n",
    "   - Fact 2 [Source#]\n",
    "   Summary: [Synthesis of sources]\n",
    "4. Never invent unavailable information</s>\n",
    "\n",
    "<|user|>\n",
    "Question: {question}</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "            # Generate response\n",
    "            response = self.llm(\n",
    "                prompt=prompt,\n",
    "                max_tokens=512,\n",
    "                temperature=0.3,\n",
    "                stop=[\"\\n\", \"###\"]\n",
    "            )\n",
    "            \n",
    "            answer = response['choices'][0]['text'].strip()\n",
    "            \n",
    "            return {\n",
    "                'question': question,\n",
    "                'answer': answer,\n",
    "                'sources': [self.vector_db.metadata[i] for i in used_indices],\n",
    "                'metrics': {\n",
    "                    'tokens_sec': len(answer.split()) / (time.time() - start_time),\n",
    "                    'response_time': time.time() - start_time\n",
    "                }\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Q&A failed: {str(e)}\")\n",
    "            return {'error': str(e)}\n",
    "\n",
    "    def translate_content(self, text: str, target_lang: str) -> str:\n",
    "        \"\"\"Bilingual translation with format preservation\"\"\"\n",
    "        try:\n",
    "            if \"[TABLE START]\" in text:\n",
    "                return self._translate_table(text, target_lang)\n",
    "                \n",
    "            # Split and translate sentences\n",
    "            sentences = nltk.sent_tokenize(text)\n",
    "            translated = []\n",
    "            \n",
    "            for sent in sentences:\n",
    "                response = self.llm(\n",
    "                    prompt=f\"<|system|>Translate to {target_lang} preserving technical terms:</s>\\n<|user|>{sent}</s>\\n<|assistant|>\",\n",
    "                    max_tokens=len(sent)*3,\n",
    "                    temperature=0.1\n",
    "                )\n",
    "                translated.append(response['choices'][0]['text'].strip())\n",
    "                \n",
    "            return ' '.join(translated)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Translation failed: {str(e)}\")\n",
    "            return text\n",
    "\n",
    "    def _translate_table(self, table_text: str, target_lang: str) -> str:\n",
    "        \"\"\"Structure-aware table translation\"\"\"\n",
    "        try:\n",
    "            rows = table_text.split('\\n')\n",
    "            translated_rows = []\n",
    "            \n",
    "            for row in rows:\n",
    "                if row.strip() in ['[TABLE START]', '[TABLE END]']:\n",
    "                    translated_rows.append(row)\n",
    "                    continue\n",
    "                    \n",
    "                cells = row.split('|')\n",
    "                translated_cells = []\n",
    "                \n",
    "                for cell in cells:\n",
    "                    response = self.llm(\n",
    "                        prompt=f\"<|system|>Translate table cell to {target_lang}:</s>\\n<|user|>{cell}</s>\\n<|assistant|>\",\n",
    "                        max_tokens=len(cell)*3,\n",
    "                        temperature=0.1\n",
    "                    )\n",
    "                    translated_cells.append(response['choices'][0]['text'].strip())\n",
    "                \n",
    "                translated_rows.append('|'.join(translated_cells))\n",
    "                \n",
    "            return '\\n'.join(translated_rows)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Table translation failed: {str(e)}\")\n",
    "            return table_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e646ff8d-4ad8-4353-b174-3ab26f2afd98",
   "metadata": {},
   "source": [
    "### 5. Translation System  \n",
    "**üöß Advanced Prototype** | *Key Components: Structure Preservation, BLEU Metrics*\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Input Text] --> B{Contains Tables?}\n",
    "    B -->|Yes| C[Table Translation Pipeline]\n",
    "    B -->|No| D[Text Translation Pipeline]\n",
    "    C --> E[Structure Parsing]\n",
    "    E --> F[Cell-by-Cell Translation]\n",
    "    F --> G[Structure Reassembly]\n",
    "    D --> H[Paragraph Segmentation]\n",
    "    H --> I[Context-Aware Translation]\n",
    "    G --> J[Output Validation]\n",
    "    I --> J\n",
    "    J --> K[BLEU Scoring]\n",
    "    K --> L((Translated Text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7938226a-55cc-48fc-a114-b2ee4f15544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationSystem:\n",
    "    def __init__(self):\n",
    "        self.llm = Llama(model_path=\"models/ggml-model-Q4_K_M.gguf\")\n",
    "        self.bleu = nltk.translate.bleu_score.SentenceBleu\n",
    "        \n",
    "    def translate(self, text: str, target_lang: str) -> Dict:\n",
    "        # Structure preservation implementation\n",
    "        translated_blocks = []\n",
    "        bleu_scores = []\n",
    "        \n",
    "        for block in self._segment_blocks(text):\n",
    "            if '[TABLE START]' in block:\n",
    "                translated = self._translate_table(block, target_lang)\n",
    "            else:\n",
    "                translated = self._translate_text(block, target_lang)\n",
    "                \n",
    "            translated_blocks.append(translated)\n",
    "            bleu_scores.append(self._calculate_bleu(block, translated))\n",
    "            \n",
    "        return {\n",
    "            'translation': '\\n'.join(translated_blocks),\n",
    "            'avg_bleu': np.mean(bleu_scores)\n",
    "        }\n",
    "\n",
    "    def _segment_blocks(self, text: str) -> List[str]:\n",
    "        # Improved structure segmentation\n",
    "        return re.split(r'(\\[TABLE START\\].*?\\[TABLE END\\])', text, flags=re.DOTALL)\n",
    "\n",
    "    def _translate_table(self, table_text: str, target_lang: str) -> str:\n",
    "        # Preserve table structure\n",
    "        rows = [row.split('|') for row in table_text.split('\\n') \n",
    "               if row.strip() not in ['[TABLE START]', '[TABLE END]']]\n",
    "        \n",
    "        translated_rows = []\n",
    "        for row in rows:\n",
    "            translated_row = [self._translate_text(cell, target_lang) for cell in row]\n",
    "            translated_rows.append('|'.join(translated_row))\n",
    "            \n",
    "        return '[TABLE START]\\n' + '\\n'.join(translated_rows) + '\\n[TABLE END]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819b0139-b310-4eba-8a9d-54d224b3b543",
   "metadata": {},
   "source": [
    "## Experimental Features\n",
    "| Approach       | Metrics           | Performance       |\n",
    "|----------------|-------------------|-------------------|\n",
    "| Abstractive    | ROUGE-L           | 0.42 F1-Score     |\n",
    "| Extractive     | Compression Ratio | 5:1               |\n",
    "| Hybrid         | BERTScore         | 0.68              |\n",
    "\n",
    "## Technical Constraints\n",
    "- Limited to 2k token inputs\n",
    "- Requires GPU for >500pg docs\n",
    "- Single-document focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee416fe0-3bd4-4637-8e78-4b647413ce4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryGenerator:\n",
    "    def __init__(self):\n",
    "        self.llm = Llama(model_path=\"models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\")\n",
    "        self.rouge = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "        \n",
    "    def generate_summary(self, text: str, strategy: str = \"map_reduce\") -> Dict:\n",
    "        if strategy == \"abstractive\":\n",
    "            prompt = f\"Generate a concise summary of the following text while preserving key technical details:\\n{text}\"\n",
    "            response = self.llm(prompt=prompt, max_tokens=512, temperature=0.3)\n",
    "            summary = response['choices'][0]['text']\n",
    "        else:  # Default extractive\n",
    "            chunks = [text[i:i+2000] for i in range(0, len(text), 2000)]\n",
    "            summaries = [self.llm(f\"Summarize this chunk: {chunk}\")['choices'][0]['text'] for chunk in chunks]\n",
    "            summary = ' '.join(summaries)\n",
    "            \n",
    "        return {\n",
    "            'summary': summary,\n",
    "            'rouge_score': self.rouge.score(text, summary)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8901bbff-347a-41e7-a333-bdba9703c2a4",
   "metadata": {},
   "source": [
    "## Real-Time Dashboard  \n",
    "```python\n",
    "{\n",
    "  \"throughput\": {\n",
    "    \"embedding\": \"385 tokens/sec\",\n",
    "    \"inference\": \"42 tokens/sec\",\n",
    "    \"retrieval\": \"1.2ms/query\"\n",
    "  },\n",
    "  \"reliability\": {\n",
    "    \"error_rate\": \"1.05%\",\n",
    "    \"retry_success\": \"82%\",\n",
    "    \"avg_uptime\": \"99.7%\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d50a7845-2e6a-4c54-ad26-5a9d6895dc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceMonitor:\n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            'embedding': [],\n",
    "            'rag': [],\n",
    "            'translation': [],\n",
    "            'summarization': []\n",
    "        }\n",
    "        \n",
    "    def track(self, operation: str, tokens: int, duration: float):\n",
    "        self.metrics[operation].append({\n",
    "            'tokens_sec': tokens / duration,\n",
    "            'timestamp': time.time()\n",
    "        })\n",
    "        \n",
    "    def report(self):\n",
    "        return {k: {\n",
    "            'avg_tokens_sec': np.mean([m['tokens_sec'] for m in v]),\n",
    "            'total_operations': len(v)\n",
    "        } for k, v in self.metrics.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e58094-dea7-48e6-ba31-b1477962af6d",
   "metadata": {},
   "source": [
    "### Main Execution Flow\n",
    "##### **Features**:\n",
    "##### - Interactive Q&A interface\n",
    "##### - File processing pipeline\n",
    "##### - Error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32771281-1c3d-4c1c-b82b-97d08232c0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 23:13:22,360 - INFO - Use pytorch device_name: cpu\n",
      "2025-04-15 23:13:22,361 - INFO - Load pretrained SentenceTransformer: nomic-ai/nomic-embed-text-v1\n",
      "2025-04-15 23:13:26,249 - WARNING - !!!!!!!!!!!!megablocks not available, using torch.matmul instead\n",
      "2025-04-15 23:13:28,502 - WARNING - <All keys matched successfully>\n",
      "2025-04-15 23:13:30,145 - INFO - Initializing LLM: tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\n",
      "2025-04-15 23:13:30,292 - INFO - Processing Dataset summaries and citations.docx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Initializing Document Processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3bdbd39220d4b98bc07d2c87bb89f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 23:14:50,398 - INFO - Added 101 chunks\n",
      "2025-04-15 23:14:50,399 - INFO - Processing Loan amortisation schedule1.xlsx\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4000084e0c394e4daab3accaf7b33289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 23:14:53,311 - INFO - Added 1 chunks\n",
      "2025-04-15 23:14:53,313 - INFO - Processing Loan analysis.xlsx\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e51ea7b715472c9e6da19e9164edce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 23:15:11,938 - INFO - Added 2 chunks\n",
      "2025-04-15 23:15:11,939 - INFO - Processing M.Sc. Applied Psychology.docx\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4130848ac7480baaa774ac2afa30d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 23:17:11,620 - INFO - Added 1981 chunks\n",
      "2025-04-15 23:17:11,622 - INFO - Processing new-approaches-and-procedures-for-cancer-treatment.pdf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06c1d6648724d69a38415537956c3e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 23:18:37,588 - INFO - Added 439 chunks\n",
      "2025-04-15 23:18:37,590 - INFO - Processing Ocean_ecogeochemistry_A_review.pdf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ca69fda4ab4f9bb4edc56013d3bdf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/77 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 23:21:35,067 - INFO - Added 2460 chunks\n",
      "2025-04-15 23:21:35,068 - INFO - Processing party budget1.xlsx\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a5a8b7cdae84c7ba1afe9bbcd9d6cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 23:21:41,542 - INFO - Added 1 chunks\n",
      "2025-04-15 23:21:41,543 - INFO - Processing Stats.docx\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30895c239a04757a84067bfc182210e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 23:22:00,104 - INFO - Added 8 chunks\n",
      "2025-04-15 23:22:00,112 - INFO - Processing The-Alchemist.pdf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb26d2e13314b31aaaeb2301c819184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 23:24:56,810 - INFO - Added 2621 chunks\n",
      "2025-04-15 23:24:56,810 - INFO - Processing The_Plan_of_the_Giza_Pyramids.pdf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4d0d93ff2b4501b5411f4240e988df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 23:26:19,741 - INFO - Added 147 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Processed 10 documents | Chunks: 7761\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Question (Enter to exit):  Tell Me About Giza Pyramids\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Analyzing...\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565c1e3f2e5a4bfb98281a87b9fa8841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìú Question: Tell Me About Giza Pyramids\n",
      "\n",
      "üí° Answer:\n",
      "Question: What is the plan for the Giza Pyramids, and how were the pyramid bases\n",
      "and components of spacing between them marked out on the ground?\n",
      "\n",
      "üîç Top Sources:\n",
      "1. The_Plan_of_the_Giza_Pyramids.pdf (pages 5\n",
      "   Glen Dash, 'Where, Precisely, are the Three Pyramids of Giza?'.\n",
      "--------------------------------------------------------------------------------\n",
      "2. The_Plan_of_the_Giza_Pyramids.pdf (pages 7\n",
      "   === PAGE 7 ===\\nThe Plan of the Giza Pyramids 7 the Great Pyramid.\n",
      "--------------------------------------------------------------------------------\n",
      "3. The_Plan_of_the_Giza_Pyramids.pdf (pages 11\n",
      "   Only the pyramid bases and the components of spacing between them had to be marked out on the ground of the Giza...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚ö° Performance: 9.2 tokens/sec\n",
      "‚è±Ô∏è Response time: 4.03s\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Question (Enter to exit):  What is the Ocean_ecogeochemistry?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Analyzing...\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b02d0360c7be4de18c425174d796d773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìú Question: What is the Ocean_ecogeochemistry?\n",
      "\n",
      "üí° Answer:\n",
      "The Ocean_ecogeochemistry refers to the study of the interactions between the\n",
      "ocean and its ecosystems, including the chemistry and biology of marine\n",
      "organisms, the movement of nutrients and other elements, and the effects of\n",
      "human activities on the marine environment. It is a field of study that combines\n",
      "oceanography, geochemistry, and biology, and is focused on understanding the\n",
      "complex relationships between the ocean and its inhabitants.\n",
      "\n",
      "üîç Top Sources:\n",
      "1. Ocean_ecogeochemistry_A_review.pdf (pages 16\n",
      "   more recently, ocean ecogeochemistry applica- tions have focused on inferring movement patterns of ish and...\n",
      "--------------------------------------------------------------------------------\n",
      "2. Ocean_ecogeochemistry_A_review.pdf (pages 47\n",
      "   Oceanographic ecology of coral reefs: the role of oceanographic processes in reef- level biogeochmistry and trophic...\n",
      "--------------------------------------------------------------------------------\n",
      "3. Ocean_ecogeochemistry_A_review.pdf (pages 39\n",
      "   Biogeochemistry doi:10.1007/s10533-011-9630-y.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚ö° Performance: 15.9 tokens/sec\n",
      "‚è±Ô∏è Response time: 6.80s\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Question (Enter to exit):  what is the cancer-treatment?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Analyzing...\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "addaf9420437469cbb33712826fe45c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìú Question: what is the cancer-treatment?\n",
      "\n",
      "üí° Answer:\n",
      "The cancer-treatment refers to the various treatments used to cure or manage\n",
      "cancer. It includes chemotherapy, radiation therapy, immunotherapy, targeted\n",
      "therapy, and other forms of cancer treatment. These treatments are used to kill\n",
      "cancer cells, reduce the size or spread of the cancer, or prevent cancer from\n",
      "growing.\n",
      "\n",
      "üîç Top Sources:\n",
      "1. new-approaches-and-procedures-for-cancer-treatment.pdf (pages 8\n",
      "   cancer treatment.\n",
      "--------------------------------------------------------------------------------\n",
      "2. new-approaches-and-procedures-for-cancer-treatment.pdf (pages 9\n",
      "   Targeted cancer therapies.\n",
      "--------------------------------------------------------------------------------\n",
      "3. new-approaches-and-procedures-for-cancer-treatment.pdf (pages 8\n",
      "   Assessment of the evolution of cancer treatment therapies.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚ö° Performance: 15.8 tokens/sec\n",
      "‚è±Ô∏è Response time: 4.62s\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Question (Enter to exit):  what is the Giza Pyramids?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Analyzing...\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fff338db9cc4a5db56cc5dba387c25c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìú Question: what is the Giza Pyramids?\n",
      "\n",
      "üí° Answer:\n",
      "The Giza Pyramids are a collection of three pyramids located in the town of\n",
      "Giza, located in the Western Desert of Egypt. The pyramids were constructed\n",
      "during the reign of King Khufu (Cheops) of the 4th dynasty (2575-2525 BCE) and\n",
      "were used as tombs for his successors. The pyramids are located in a vast\n",
      "necropolis that was also used for the burials of the rulers of the 4th dynasty.\n",
      "The pyramids were built using the same design principles as the Great Pyramid of\n",
      "Giza, which was constructed during the reign of Khufu's father, Pharaoh\n",
      "Djedefre.\n",
      "\n",
      "üîç Top Sources:\n",
      "1. The_Plan_of_the_Giza_Pyramids.pdf (pages 5\n",
      "   Glen Dash, 'Where, Precisely, are the Three Pyramids of Giza?'.\n",
      "--------------------------------------------------------------------------------\n",
      "2. The_Plan_of_the_Giza_Pyramids.pdf (pages 7\n",
      "   === PAGE 7 ===\\nThe Plan of the Giza Pyramids 7 the Great Pyramid.\n",
      "--------------------------------------------------------------------------------\n",
      "3. The_Plan_of_the_Giza_Pyramids.pdf (pages 15\n",
      "   There is thus a hint of the further relationships which are hidden within structure of the plan.19 Conclusion Whilst...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚ö° Performance: 16.7 tokens/sec\n",
      "‚è±Ô∏è Response time: 9.50s\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Question (Enter to exit):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîö Session ended\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "import textwrap\n",
    "import logging\n",
    "import warnings\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Other imports (ResearchProcessor, ResearchChunker, etc.) remain the same\n",
    "\n",
    "def analyze_research_documents(input_dir: Path):\n",
    "    \"\"\"Enhanced document analysis with robust error handling and user feedback\"\"\"\n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            processor = ResearchProcessor()\n",
    "            chunker = ResearchChunker()\n",
    "            analyst = ResearchAnalyst()\n",
    "            \n",
    "            print(\"\\nüîç Initializing Document Processing...\")\n",
    "            documents = []\n",
    "            \n",
    "            file_list = list(input_dir.glob('*'))\n",
    "            if not file_list:\n",
    "                raise ValueError(\"No files found in input directory\")\n",
    "                \n",
    "            # Suppress output during processing\n",
    "            with redirect_stdout(io.StringIO()) as out, redirect_stderr(io.StringIO()) as err:\n",
    "                for file_path in tqdm(file_list, desc=\"Processing documents\"):\n",
    "                    try:\n",
    "                        if file_path.suffix.lower() not in ResearchProcessor.SUPPORTED_FORMATS:\n",
    "                            continue\n",
    "                        \n",
    "                        result = processor.process_document(file_path)\n",
    "                        if not result or not isinstance(result.get('content'), list):\n",
    "                            continue\n",
    "                        \n",
    "                        # Validate document structure\n",
    "                        valid_content = [\n",
    "                            elem for elem in result['content']\n",
    "                            if isinstance(elem, dict) and 'text' in elem\n",
    "                        ]\n",
    "                        if not valid_content:\n",
    "                            continue\n",
    "                        \n",
    "                        result['content'] = valid_content\n",
    "                        chunks = chunker.chunk_document(result)\n",
    "                        \n",
    "                        if chunks:\n",
    "                            try:\n",
    "                                analyst.vector_db.add_documents(chunks)\n",
    "                                documents.append(result)\n",
    "                            except Exception as db_error:\n",
    "                                logger.error(f\"Vector DB insertion failed: {str(db_error)}\")\n",
    "                                continue\n",
    "                                \n",
    "                    except Exception as doc_error:\n",
    "                        logger.error(f\"Failed processing {file_path.name}: {str(doc_error)}\")\n",
    "                        continue\n",
    "            \n",
    "            print(f\"\\n‚úÖ Processed {len(documents)} documents | Chunks: {len(analyst.vector_db.metadata)}\")\n",
    "            \n",
    "            # Interactive Q&A session\n",
    "            while True:\n",
    "                try:\n",
    "                    question = input(\"\\nüìù Question (Enter to exit): \").strip()\n",
    "                    if not question:\n",
    "                        print(\"\\nüîö Session ended\")\n",
    "                        break\n",
    "                    \n",
    "                    print(\"üîÑ Analyzing...\", end='\\r')\n",
    "                    start_time = time.time()\n",
    "                    result = analyst.answer_question(question)\n",
    "                    \n",
    "                    # Display results\n",
    "                    print(\"\\n\" + \"=\"*80)\n",
    "                    print(f\"üìú Question: {question}\")\n",
    "                    \n",
    "                    if 'error' in result:\n",
    "                        print(f\"‚ùå {result['error']}\")\n",
    "                        continue\n",
    "                    \n",
    "                    print(f\"\\nüí° Answer:\\n{textwrap.fill(result['answer'], width=80)}\")\n",
    "                    \n",
    "                    if result['sources']:\n",
    "                        print(\"\\nüîç Top Sources:\")\n",
    "                        for i, src in enumerate(result['sources'], 1):\n",
    "                            pages = f\" (pages {', '.join(map(str, src['page_numbers']))}\" \\\n",
    "                                  if src['page_numbers'] else \"\"\n",
    "                            print(f\"{i}. {src['document_id']}{pages}\")\n",
    "                            print(f\"   {textwrap.shorten(src['content'], width=120, placeholder='...')}\")\n",
    "                            print(\"-\"*80)\n",
    "                    \n",
    "                    # Calculate performance metrics\n",
    "                    tokens = len(analyst.llm.tokenize(result['answer'].encode()))\n",
    "                    tokens_sec = tokens / (time.time() - start_time)\n",
    "                    print(f\"\\n‚ö° Performance: {tokens_sec:.1f} tokens/sec\")\n",
    "                    print(f\"‚è±Ô∏è Response time: {time.time()-start_time:.2f}s\")\n",
    "                    print(\"=\"*80)\n",
    "                    \n",
    "                except KeyboardInterrupt:\n",
    "                    print(\"\\nüõë Session cancelled\")\n",
    "                    break\n",
    "                    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Critical error: {str(e)}\")\n",
    "        print(\"\\n‚ùå Fatal error - check logs\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        base_dir = Path(r\"D:\\Projects\\The Enigmatic Research of Dr. X\")\n",
    "        input_dir = base_dir / \"documents\"\n",
    "        \n",
    "        if not input_dir.exists():\n",
    "            raise FileNotFoundError(f\"Directory missing: {input_dir}\")\n",
    "            \n",
    "        analyze_research_documents(input_dir)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Startup failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b666590e-920e-421f-9f81-ea86c6bf0c02",
   "metadata": {},
   "source": [
    "# we can improve the model using llama not Tiny but at all it's good results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
